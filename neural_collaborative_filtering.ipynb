{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOfjsoZezwtrwPE2HOljtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonsang010716/Recommendation-system/blob/main/neural_collaborative_filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXNUJ8JxgWif",
        "outputId": "891d705d-2d82-4e56-f04e-49f8b7349d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "#gpu 사용하기\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip ml-latest-small.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-6sxdpJh2Pz",
        "outputId": "ed5ce7b1-6efa-4d1d-d76a-06ccce3623d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-07 05:30:57--  https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 978202 (955K) [application/zip]\n",
            "Saving to: ‘ml-latest-small.zip’\n",
            "\n",
            "ml-latest-small.zip 100%[===================>] 955.28K  1.54MB/s    in 0.6s    \n",
            "\n",
            "2024-05-07 05:30:58 (1.54 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
            "\n",
            "Archive:  ml-latest-small.zip\n",
            "   creating: ml-latest-small/\n",
            "  inflating: ml-latest-small/links.csv  \n",
            "  inflating: ml-latest-small/tags.csv  \n",
            "  inflating: ml-latest-small/ratings.csv  \n",
            "  inflating: ml-latest-small/README.txt  \n",
            "  inflating: ml-latest-small/movies.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = []\n",
        "items = []\n",
        "ratings = []\n",
        "with open(\"ml-latest-small/ratings.csv\" , \"r\") as f:\n",
        "  f.readline()\n",
        "  for line in f:\n",
        "    uid , mid , rating , _ = line.split(\",\")\n",
        "    users.append(int(uid))\n",
        "    items.append(int(mid))\n",
        "    ratings.append(float(rating))\n",
        "\n",
        "  users = torch.tensor(users)\n",
        "  items = torch.tensor(items)\n",
        "  ratings = torch.tensor(ratings)\n",
        "  print(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE-Ard5xiMNp",
        "outputId": "912b681a-93a3-4483-9384-d7ec3206a462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4., 4., 4.,  ..., 5., 5., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_nnz = len(users)\n",
        "train_size = int(n_nnz * 0.8)\n",
        "shuffled_indices = torch.randperm(n_nnz)\n",
        "#그냥 저장하면 main memory에 저장되기 때문에 gpu를 사용하기 위해서 gpu 상에 올려줘야한다.\n",
        "users_train = users[shuffled_indices[:train_size]].to(device)\n",
        "items_train = items[shuffled_indices[:train_size]].to(device)\n",
        "ratings_train = ratings[shuffled_indices[:train_size]].to(device)\n",
        "\n",
        "users_test = users[shuffled_indices[train_size:]].to(device)\n",
        "items_test = items[shuffled_indices[train_size:]].to(device)\n",
        "ratings_test = ratings[shuffled_indices[train_size:]].to(device)\n"
      ],
      "metadata": {
        "id": "Bn7iVYyhjO7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "#neural collaborative filtering\n",
        "# user vector 와 item vector를 concat 해서 mult layer n\n",
        "n_users = max(users) + 1\n",
        "n_items = max(items) + 1\n",
        "n_factors = 10\n",
        "dropout_rate = 0.4\n",
        "P = torch.randn(n_users , n_factors , requires_grad = True , device = device)\n",
        "Q = torch.randn(n_items , n_factors , requires_grad = True , device = device)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_factors*2 , 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(dropout_rate),\n",
        "    nn.Linear(10,5),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(dropout_rate),\n",
        "    nn.Linear(5,1)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "8GjdPbRdkTMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = nn.MSELoss()\n",
        "optim = torch.optim.Adam(list(model.parameters()) + [P,Q], lr = 0.1)\n",
        "for epoch in range(3000):\n",
        "  model.train()\n",
        "  x = torch.cat((P[users_train] , Q[items_train] ), dim = 1)\n",
        "  h = model(x).squeeze()\n",
        "  cost = mse(h , ratings_train)\n",
        "  optim.zero_grad()\n",
        "  cost.backward()\n",
        "  optim.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 100 == 0:\n",
        "      model.eval()\n",
        "      x_train = torch.cat((P[users_train] , Q[items_train] ), dim = 1)\n",
        "      h_train = model(x_train).squeeze()\n",
        "      train_mse = mse(h_train , ratings_train)\n",
        "\n",
        "      x_test = torch.cat((P[users_test] , Q[items_test] ), dim = 1)\n",
        "      h_test = model(x_test).squeeze()\n",
        "      test_mse = mse(h_test , ratings_test)\n",
        "      print(f\"epoch : {epoch} , train_mse: {train_mse.item()} , test_mse : {test_mse.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6SwzFb9oydq",
        "outputId": "4ab7b83c-7db1-4a8e-c8f9-bbb83247e91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 , train_mse: 1.294806957244873 , test_mse : 1.3332263231277466\n",
            "epoch : 100 , train_mse: 0.7022123336791992 , test_mse : 0.834744930267334\n",
            "epoch : 200 , train_mse: 0.6876100301742554 , test_mse : 0.8356665372848511\n",
            "epoch : 300 , train_mse: 0.6808602809906006 , test_mse : 0.8362711071968079\n",
            "epoch : 400 , train_mse: 0.6733590960502625 , test_mse : 0.8358569145202637\n",
            "epoch : 500 , train_mse: 0.6734362840652466 , test_mse : 0.8389440774917603\n",
            "epoch : 600 , train_mse: 0.6725870370864868 , test_mse : 0.8370033502578735\n",
            "epoch : 700 , train_mse: 0.6759024262428284 , test_mse : 0.8371350169181824\n",
            "epoch : 800 , train_mse: 0.670719563961029 , test_mse : 0.836831271648407\n",
            "epoch : 900 , train_mse: 0.673116147518158 , test_mse : 0.838914692401886\n",
            "epoch : 1000 , train_mse: 0.6692038774490356 , test_mse : 0.8357536792755127\n",
            "epoch : 1100 , train_mse: 0.6737407445907593 , test_mse : 0.8412884473800659\n",
            "epoch : 1200 , train_mse: 0.6636024713516235 , test_mse : 0.837451159954071\n",
            "epoch : 1300 , train_mse: 0.6663428544998169 , test_mse : 0.839267909526825\n",
            "epoch : 1400 , train_mse: 0.6669549345970154 , test_mse : 0.8365222215652466\n",
            "epoch : 1500 , train_mse: 0.6730601787567139 , test_mse : 0.8389726281166077\n",
            "epoch : 1600 , train_mse: 0.6909903883934021 , test_mse : 0.8500048518180847\n",
            "epoch : 1700 , train_mse: 0.675536036491394 , test_mse : 0.843230128288269\n",
            "epoch : 1800 , train_mse: 0.6613385677337646 , test_mse : 0.8384209871292114\n",
            "epoch : 1900 , train_mse: 0.664917528629303 , test_mse : 0.8402489423751831\n",
            "epoch : 2000 , train_mse: 0.6700069308280945 , test_mse : 0.8423333764076233\n",
            "epoch : 2100 , train_mse: 0.6584655046463013 , test_mse : 0.8392791152000427\n",
            "epoch : 2200 , train_mse: 0.6726697087287903 , test_mse : 0.8418477773666382\n",
            "epoch : 2300 , train_mse: 0.662834107875824 , test_mse : 0.8420641422271729\n",
            "epoch : 2400 , train_mse: 0.6696768403053284 , test_mse : 0.842547595500946\n",
            "epoch : 2500 , train_mse: 0.6985125541687012 , test_mse : 0.8521192073822021\n",
            "epoch : 2600 , train_mse: 0.6643989682197571 , test_mse : 0.8417732119560242\n",
            "epoch : 2700 , train_mse: 0.6558469533920288 , test_mse : 0.8385123610496521\n",
            "epoch : 2800 , train_mse: 0.6810460686683655 , test_mse : 0.8451849818229675\n",
            "epoch : 2900 , train_mse: 0.6722438335418701 , test_mse : 0.8417614102363586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "rating_matrix_train = torch.zeros(n_users , n_items)\n",
        "for uid , mid , rating in zip(users_train , items_train , ratings_train):\n",
        "  rating_matrix_train[uid , mid] = rating\n",
        "rating_matrix_train = rating_matrix_train.to(device)\n",
        "\n",
        "rating_matrix_test = torch.zeros(n_users , n_items)\n",
        "for uid , mid , rating in zip(users_test , items_test , ratings_test):\n",
        "  rating_matrix_test[uid , mid] = rating\n",
        "rating_matrix_test = rating_matrix_test.to(device)"
      ],
      "metadata": {
        "id": "febkdz2Aun7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_factors = 30\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_users , n_factors),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(n_factors , n_users)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "CBeh_JX3uxGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters() , lr = 0.1 , weight_decay = 0.001)\n",
        "nnz_train = rating_matrix_train != 0\n",
        "nnz_test = rating_matrix_test != 0\n",
        "for epoch in range(30):\n",
        "  h = model(rating_matrix_train.T)\n",
        "  cost = mse(h.T[nnz_train] , rating_matrix_train[nnz_train])\n",
        "  optim.zero_grad()\n",
        "  cost.backward()\n",
        "  optim.step()\n",
        "  with torch.no_grad():\n",
        "    h_train = model(rating_matrix_train.T)\n",
        "    train_mse = mse(h_train.T[nnz_train] , rating_matrix_train[nnz_train])\n",
        "\n",
        "    h_test = model(rating_matrix_test.T)\n",
        "    test_mse = mse(h_test.T[nnz_test] , rating_matrix_test[nnz_test])\n",
        "    print(f\"epoch : {epoch} , train_mse : {train_mse.item()} , test_mse : {test_mse.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Q5aXlpvhB7",
        "outputId": "95496bad-4381-4071-c446-b450ec0626d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 , train_mse : 6.065615177154541 , test_mse : 5.773454189300537\n",
            "epoch : 1 , train_mse : 1.6659064292907715 , test_mse : 1.6142293214797974\n",
            "epoch : 2 , train_mse : 1.7187237739562988 , test_mse : 1.704734444618225\n",
            "epoch : 3 , train_mse : 2.878044366836548 , test_mse : 2.969487190246582\n",
            "epoch : 4 , train_mse : 2.9270591735839844 , test_mse : 3.0922937393188477\n",
            "epoch : 5 , train_mse : 2.361156940460205 , test_mse : 2.465439558029175\n",
            "epoch : 6 , train_mse : 1.7006028890609741 , test_mse : 1.7586747407913208\n",
            "epoch : 7 , train_mse : 1.2922803163528442 , test_mse : 1.3429473638534546\n",
            "epoch : 8 , train_mse : 1.2224193811416626 , test_mse : 1.2848525047302246\n",
            "epoch : 9 , train_mse : 1.3712282180786133 , test_mse : 1.4530433416366577\n",
            "epoch : 10 , train_mse : 1.5603132247924805 , test_mse : 1.6544336080551147\n",
            "epoch : 11 , train_mse : 1.6455957889556885 , test_mse : 1.7417880296707153\n",
            "epoch : 12 , train_mse : 1.5861457586288452 , test_mse : 1.6726640462875366\n",
            "epoch : 13 , train_mse : 1.430887222290039 , test_mse : 1.4956687688827515\n",
            "epoch : 14 , train_mse : 1.2579395771026611 , test_mse : 1.2929399013519287\n",
            "epoch : 15 , train_mse : 1.1263729333877563 , test_mse : 1.1293015480041504\n",
            "epoch : 16 , train_mse : 1.0548248291015625 , test_mse : 1.0286242961883545\n",
            "epoch : 17 , train_mse : 1.028798222541809 , test_mse : 0.9812959432601929\n",
            "epoch : 18 , train_mse : 1.019534945487976 , test_mse : 0.9636086821556091\n",
            "epoch : 19 , train_mse : 1.0033901929855347 , test_mse : 0.9570094347000122\n",
            "epoch : 20 , train_mse : 0.9710528254508972 , test_mse : 0.9590827226638794\n",
            "epoch : 21 , train_mse : 0.9409817457199097 , test_mse : 0.9910562634468079\n",
            "epoch : 22 , train_mse : 0.9462778568267822 , test_mse : 1.0730559825897217\n",
            "epoch : 23 , train_mse : 0.9622951149940491 , test_mse : 1.1467721462249756\n",
            "epoch : 24 , train_mse : 0.9550474882125854 , test_mse : 1.1597098112106323\n",
            "epoch : 25 , train_mse : 0.9204227328300476 , test_mse : 1.1240142583847046\n",
            "epoch : 26 , train_mse : 0.871099054813385 , test_mse : 1.0601441860198975\n",
            "epoch : 27 , train_mse : 0.8390929698944092 , test_mse : 0.9957640767097473\n",
            "epoch : 28 , train_mse : 0.8358414173126221 , test_mse : 0.9571391940116882\n",
            "epoch : 29 , train_mse : 0.845636785030365 , test_mse : 0.9413663148880005\n"
          ]
        }
      ]
    }
  ]
}